{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkhEayQxY8GqgFrabzZu3T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamburins/ESAA_2023/blob/main/ESAA_4_7(466_486).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8 텍스트 분석\n",
        "\n",
        "### NLP이냐 텍스트 분석이냐\n",
        "NLP란 머신이 인간의 언어를 이해하고 해석하는데 중점을 두고 기술이 발전해 왔으며, 텍스트 마이닝은 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점을 둔 것이다.\n",
        "\n",
        "- NLP는 언어를 해석하기 위한 기계번역, 자동으로 질문을 해석하고 답을 해주는 질의응답 시스템 등의 영역 등에서 텍스트 분석과 차별점이 존재한다. 이 기술이 발전함에 따라 텍스트 분석도 같이 발전하고 발전 가능해졌다.\n",
        "\n",
        "- 머신러닝, 언어이해, 통계등을 활용해 모델을 수립하고 정보를 추출해 비스니스 인텔리전스나 예측 분석등의 분석 작업을 주로 수행한다. 머신러닝 기술에 힘입어 텍스트분석은 크게 발전하고 있으며 주로\n",
        "\n",
        "> 텍스트분류: 문서가 특정 분류 혹은 카테고리에 속하는 것을 예측하는 기법을 통칭하는 것으로 기사의 정치, 연예, 사회, 문화 카테고리를 정하거나 스팸메일 검출 프로그램등이 속한다.\n",
        "\n",
        "> 감성 분석: 텍스트에서 나타내는 감정판단믿음의견기분등의 주관적인 요소를 분석하는 기법의 총칭으로 소셜미디어 감정분석, 영화나 제품에 대한 긍정 또는 리뷰, 여론조사 등에서 다양하게 활용\n",
        "\n",
        "> 텍스트 군집화: 시슷한 유형의 문서에 대해 군집화를 수행하는 기법으로 텍스트 분류를 비지도학습으로 수행하는 방법이며 유사도 측정으로 이를 수행가능\n",
        "\n",
        "> 텍스트 요약: 텍스트 내에서 중요한 주제나 중심사상을 추출하는 기법으로 대표적으로 토픽 모델링이 있다\n",
        "\n",
        "등이 있다.\n",
        "\n",
        "## 01 텍스트 분석 이해\n",
        "\n",
        "비정형 데이터인 텍스트를 분석하는 것으로 대부분의 머신러닝 모델은 주어진 정형데이터 기반에서 모델을 수립하고 예측을 수행했으며 숫자형으 ㅣ피처기반 데이터만 입력 가능하도록 했기 때문에 이것을 피처형태로 어떻게 추출하며 추출된 피처에 의미있는 값을 부여하는 것이 매우 중요한 요소이다.\n",
        "\n",
        "즉 이렇게 추출된 텍스트는 단어의 조합인 벡터값으로 표현될 수 있는데 이를 피처 벡터화 혹은 피처추출이라 하며 이의 방법에는 BOW와 word2voc 방법이 있으나 우리는 BOW만 다루고자 한다.\n",
        "\n",
        "### 텍스트 분석 수행 프로세스\n",
        "\n",
        "1. 텍스트 사전 준비작업(텍스트 전처리): 텍스트를 피처로 만들기 전에 미리 클렌징, 대소문자 변경, 특수문자 제거 등의 작업과 단어 토큰화작업, 의미없는 단어제거, 어근 추출등의 정규화작업을 수행\n",
        "\n",
        "2. 피처 벡터화/추출: 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡터값을 할당한다. BOWDHK WORD2VEC이 있으며 BOW에는 count기반과 tf-idf기반벡터화가 있다.\n",
        "\n",
        "3. ML 모델 수립 및 학습 예측 평가\n",
        "\n",
        "### 파이썬 기반 NLP, 텍스트 분석 패키지\n",
        "\n",
        "- 대표적인 패키지 NTLK는 방대한 데이터와 서브모듈, 다양한 데이터세트를지원하나 성능과 정확도 등에서 부족한 부분이 있어 Genism, SpaCy가 이를 보완하며 실 업무에서 자주 활용된다.\n",
        "\n",
        "> NLTK: 방대한 데이터세트와 서브모듈을 가지며 거의 모든 영역을 커버하나 수행속도 면에서 아쉬운 점 존재\n",
        "\n",
        "> Genism: 토픽 모델링 분야에서 가장 두각을 나타내는 패키지로 토픽 모델링을 쉽게 구현할 수 있는 기능을 제공해왔으며 word2vec등의 신기능을 제공하고 많이 사용됨\n",
        "\n",
        "> SpaCy: 뛰어난 수행성능의 NLP 패키지\n",
        "\n",
        "- 다만 어근처리에 대한 라이브러리는 없으나 텍스트를 일정수준으로 가공하고 머심러닝 알고리즘에 텍스트 데이터를 피처로 처리하기 위한 편리한 기능을 제공하고 있어 사이킷런으로도 텍스트 분석을 충분히 수행 가능\n",
        "\n",
        "## 02 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화\n",
        "\n",
        "텍스트 자체를 바로 피처화할 수 없어 가공 준비가 필요한데 먼저 텍스트 정규화는 텍스트를 머신러닝 알고리즘이나 'nlp' 어플리케이션에 입력 데이터로 사용하기 위해 클렌징, 정제, 토큰화, 어근화 등의 다양한 사전작업을 수행하는 것으로 정규화 과정은 크게\n",
        "\n",
        "- 클렌징: 방해되는 불필요한 문자, 기호등을 사전에 제거\n",
        "\n",
        "- 토큰화: 문서에서 문장을 분리하는 문장 토큰화 --와 단어를 토큰으로 분리하는 단어 토큰화 - -로 분류\n",
        "\n"
      ],
      "metadata": {
        "id": "Dm2ryPHC05xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 토큰화\n",
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "                You can see it out your window or on your television. \\\n",
        "                You feel it when you go to work, or go to church or pay your tzes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKh8sVRg9CqJ",
        "outputId": "54d3a726-9892-484d-8637-abf015b442a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your tzes.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 각각의 문장으로 구성된 리스트 객체를 반환한다.\n",
        "\n",
        "#### 단어 토큰화\n",
        "문장을 단어로 토큰화하는 것으로 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리하면 정규 표현식을 이용해 다양한 유형으로 토큰화를 수행 가능\n",
        "\n",
        "마침표나 개행문자와 같이 문장을 분리하는 구분자를 이용해 단어를 토큰화할 수 있으므로 단어의 순서가 중요하지 않은 경우 단어토큰화만 해도 괜찮다"
      ],
      "metadata": {
        "id": "FfzilRoo94CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8ZJaKj8-Syw",
        "outputId": "d16eb0e2-7f11-42d3-e854-d9ca913d1ee8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 단어 토큰화와 문장 토큰화를 조합해 문서에 대해 모든 단어를 토큰화해보고자 한다. "
      ],
      "metadata": {
        "id": "4M7sB80s-n6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "# 여러개의 문장으로 된 입력데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "  # tokenize with sentence\n",
        "  senteces = sent_tokenize(text)\n",
        "  # tokenize word from sentence\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in senteces]\n",
        "  return word_tokens\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_-PvVt--nLx",
        "outputId": "c6d6b76f-f498-4a1c-fb85-2aed837dd97c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'tzes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3개의 문장을 먼저 문장별로 토큰화하여 3개의 리스트 객체를 내포하는 리스트로 word_tokens가 가지며 내포된 개별 리스트 객체는 각각 문장별로 토큰화된 단어를 요소로 포함한다.\n",
        "\n",
        "문장을 단어별로 하나씩 토큰화할 경우 문맥적 의미는 무시되나 이를 해결하기 위해 도입된 것이 n-gram으로 연속된 n개의 단어를 하나의 토큰화 단위르 분리하는 것"
      ],
      "metadata": {
        "id": "kh8n38oY_X8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 스톱워드 제거\n",
        "- 필터링/스톱 워드 제거/철자 수정\n",
        "\n",
        "이 중 스톱워드 제거는 분석에 큰 의미가 없는 스톱워드를 제거하는 것으로 영어에서 is, the, a, will과 같이 필수 문법 요소이나 큰 의미가 없는 단어를 제거하지 않으면 오히려 빈도로 인해 중요단어로 등장할 수 있어 중요한 전처리 작업이다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CB3yltZT8-8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print('the number of English stopwords: ', len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL6R-iqC_9Rh",
        "outputId": "279b8682-a80c-4050-e505-36c46ff2efc9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of English stopwords:  179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어의 경우 스톱워드 개수가 179개이며 이 중 20개만 살펴보면 다음과 같다. 이를 바탕으로 스톱워드를 필터링으로 제거하면?"
      ],
      "metadata": {
        "id": "Lo6URJoNAP6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens:\n",
        "  filtered_words =[]\n",
        "\n",
        "  for word in sentence:\n",
        "    # 소문자 변환\n",
        "    word = word.lower()\n",
        "    # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMAoi27zAXMk",
        "outputId": "f7814503-4f44-4efb-c162-5f07b9660a2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'tzes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is, this와 같은 스톱워드가 필터링으로 제거되었다.\n",
        "\n",
        "### stemming과 lemmatization\n",
        "\n",
        "둘다 문법적으로 또는 의미적으로 변화하는 단어의 원형을 찾기 위한 것으로 lem은 stem에 비해 의미론적인 기반에서 단어의 원형을 찾으며 stem은 원형 단어로 변환시 일반적인 받ㅇ법을 적용하거나 더 단순화된 방법을 적용해 단어에서 일부 철자가 훼손된 어근 단어를 추출한다. 반면 lem은 품사와 같은 문법적 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아주기 때문에 그만큼 더 오래 걸린다.\n",
        "\n",
        "NLTK는 다양한 stemmer를 제공한다. 먼저 스테머부터 살펴보고자 한다. 이는 진행형, 3인칭 단수, 과거형에 따른 동사, 비교 등 단순하게 원형 단어를 찾아준다."
      ],
      "metadata": {
        "id": "SJTj02lR_8sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2LiHho7BqJn",
        "outputId": "3291f66e-0c88-44ed-ee1e-e23854e900c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmaization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest','a'))\n",
        "# better than before!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPwnEUnnCHVZ",
        "outputId": "a38988dd-daa5-4875-a631-a48e504a5920"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Bag of WOrds - BOW\n",
        "\n",
        "문서가 갖는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도값을 부여해 피처 값을 추출하는 모델로 모든 단어를 한거번에 봉투(bag)에 넣어 흔들어 섞는다는 의미로 bow라 부른다.\n",
        "\n",
        "\n",
        "문 장 1:\n",
        "M' y wife likes ot watch baseball games and my daughter likes towatch baseball games too'\n",
        "2:\n",
        "M' y wife likes to play baseball'\n",
        "\n",
        "라는 문장이 있을 때, bow의 단어수 기반으로 피처를 추출해보겠다,\n",
        "\n",
        "\n",
        "1. 문장 1과 문장 2에있는 모든 단어에서 중복을 제거하고 각 단어를 칼럼 형태로 나열한 뒤 각 단어에 고유 인덱스를 부여한다.\n",
        "\n",
        "2. 개별 문장에서 해당 단어가 나타나는 횟수를 각 단어에 기재한다.\n",
        "\n",
        "bow 모델의 장점은 쉽거 빠른 구축에 있으며 문서의 특징을 잘 나타낸다는 부분이나, 문맥의미 반영 부족고 ㅏ최소 행렬 문제가 있다.\n",
        "\n",
        "- 문맥의미 반영 부족: 단어 순서를 고려하지 않아 문장내의 단어의 문맥 의미가 무시되며 이를 보완하기 위해 n_gram을 활용할 수 있으나 제한적이다.\n",
        "\n",
        "- bow로 피처벡터화를 수행하면 최소행렬형태의 데이터세트가 만들어지지 않아 문서마다 서로 다른 단어로 구성되기에 단어가 문서마다 나타나지 않는 경우가 훨씬 더 많다. 즉 대부분의 데이터는 0값으로 채워지는데 이러한 것을 최소행렬이라 하고 이는 알고리즘의 수행시간과 예측성능을 악화시킨다는 단점이 있다.\n",
        "\n",
        "### BOW 피처 벡터화\n",
        "\n",
        "머신러닝 알고리즘은 일반적으로 숫자형 피처를 데이터로 입력받아 동작하기 때문에 텍스트와 같은 데이터는 머신러닝 알고리즘에 바로 입력할 수 없어 특정 의미를 갖는 숫자형 값인 벡터값으로 변환해야 하는데 이를 피처벡터화라 한다.\n",
        "\n",
        "예를 들어 각 문서의텍스트를 단어로 추출해 피처로 할당하고, 각 단어으 ㅣ발생 빈도와 같은 값을 이 피처에 값으로 부여해 각 문서를 이 단어 피처으 ㅣ발생빈도 값으로 구성된 벡터로 생성하는 기법이다.\n",
        "\n",
        "피처 벡터화는 기존 텍스트 데이터를 또 다른 형태의 피처조합으로 변경하기 때문에 넓은 범위의 피처추출에 포함하며 이는 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것이다.\n",
        "\n",
        "일반적으로 카운트기반의 벡터화, tf-idf기반의 벡터화라는 두가지 방식을 갖는다. 단어 피처에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수 즉 카운트를 부여하는 경우를 카운트 벡터화로 카운트값이 높을수록 중요단어로 인식된다. 그러나 카운트만 부여할경우 단순 횟수세기로 인해 빈도수만 따지게 되어 언어의 특성상 문장에 자주 사용되는 단어가 중요단어로 등극할 수 있다. 따라서 이를 보완하려면 tf-idf를 사용한다. 개별 문서에서 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 나타나는 단어에 대해 페널티를 주는 것이다.\n",
        "\n",
        "어떤 문서의 특정 단어가 자주 나타나면 중요 단어일 수 있으나 다른 문서에도 나타난다면 그냥 언어 특성상 자주 나타나는 단어일 확률이 높다. 이러한 단어의 등장횟수로만 중요도를 평가하면 문제가 되기 때문에 모든 문서의 반복적 단어에 대해 페널티를 부여하는 것이다.\n",
        "\n",
        "### 사이킷런의 count 및 TF-IDF 벡터화 구현: countVectorizer, TfidfVectorizer\n",
        " 사이킷런의 countvectorizer 클래스는 카운트 기반의 벡터하를 구현한 클래스로 피처벡터화만 수행하는 것이 아니라 소문자 일괄변환, 토큰화, 스톱워드 필터링 등 텍스트 전처리도 동시해 수행한다. 마찬가지로 fit과 transform으로 피처벡터화된 객체를 반환한다.\n",
        "\n",
        " > max_df: 전체 문서에 걸쳐 높은 빈도수인 단어를 필터링하기 위한 파라미터로 정수값을 가지면 그 정수값 이하로 나타나는 단어만 피처로 추출하고 소수점인 경우 전체문서에 걸쳐 빈도수가 nn퍼센트 까지의 단어만 피처로 추출하고 나머지 상위 n퍼센트는 피처로 추출하지 않는다.\n",
        "\n",
        " > min_df:전체 문서에 걸쳐 너무 낮은 빈도수의 단어를 제외하기 위한 파라미터로 너무 낮은 파라미터는 가비지 단어이거나 중요하지 않을 확률이 크다. max_df와 동일하다.\n",
        "\n",
        " > max_features: 추출 피처의 개수를 제한하며 정수로 값을 지정하여 가장 높은 빈도 순으로 상위 정수개의 피처를 추출한다\n",
        "\n",
        " > stopwords:영어로 지정하면 영어의 스톱워드로 지정된 단어는 추출에서 제외됨\n",
        "\n",
        " > n_gram_range: 단어 순서를 보강하기 위한 것으로 토큰화된 단어를 앞숫자씩 피처로 추출하고 순서대로 뒷수자씩 묶어 피처로 추출한다.\n",
        "\n",
        " > analyzer:피처 추출을 수행한 단위를 지정한다. 기본은 word이며 character으 ㅣ특정 범위를 피처로 만드는 등 특정경우에 적용한다.\n",
        "\n",
        "\n",
        " > token_pattern: 토큰화를 수행하는 정규 표현식 패턴을 지정하며 공백이나 개행문자 등 구분된 단어분리자 사이의 2문자 이상의 단어를 토큰으로 분리한다.\n",
        "\n",
        " > tokenizer: 토큰화를 별도커스텀 함수로 이용시 적용하며 일반적으로 counttokenizer 클래스에서 변환시 수행하는 별도 함수를 tokenizer 파라미터에 적용하면 된다.\n",
        "\n",
        "크게 사전데이터 가공(소문자변환)> n_gram_range 반영의 토큰화(디폴트는 anlayzer = true) > 텍스트 정규화(stop words 필터링 수행 ) > 피처 벡터화 (max_df, min_df 등 반영 )로 나뉜다.\n",
        "\n",
        "### BOW 벡터화를 위한 희소 행렬\n",
        "\n",
        "사이킷런을 이용해 텍스트를 피처단위로 벡터화해 변환하고 csr 형태의 희소행렬을 반환한다. 사용자 입장에서 피처벡터화된 희소행렬이 어떤 형태인지 중요하진 않을수 있으나, 난이도 있는 ml모델 수립을 위해서는 희소행렬의 형태파악이 필요하다.\n",
        "\n",
        "모든 문서의 단어를 추출해 피처로 벡터화하는 방법은 필연적으로 많은 피처칼럼을 만들 수밖에 없다. 모든 문서의 단어 중ㅂ고을 제거하고 피처로 만ㄷ르면 일반적으로 매우 많은 단어가 생성된다. 이런 대규모 행렬이 생성되더라도 레코드의 각 문서가 가지는 단어의 수는 제한적이기 때문에 행렬의 대부분 값은 0일 수밖에 없다. 이런 대부분의 값이 0인 행렬을 희소행렬이라 한다.\n",
        "\n",
        "이는 너무 많은 불필요한 0값이 메모리 공간에 할당되어 메모리 공간을 많이 차지하며 행렬 크기가 커 연산시에도 시간이 많이 소모되기에 이를  변환할 필요가 있는데 대표적으로 coo, csr이 있다.\n",
        "\n",
        "### 희소 행렬 - COO 형식\n",
        "\n",
        "0이 아닌 데이터만 별도의 데이터 배열에 저장하고 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식으로 scipy를 이용해 할 수 있다."
      ],
      "metadata": {
        "id": "u1YJG9ctCza-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dense = np.array([[3,0,1], [0,2,0]])"
      ],
      "metadata": {
        "id": "3zRKMOLj81sV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "# not zero data \n",
        "data = np.array([3,1,2])\n",
        "\n",
        "# 행 위치와 열 위치 각각 배열로 전달\n",
        "row_pos = np.array([0,0,1])\n",
        "col_pos = np.array([0,2,1])\n",
        "\n",
        "# sparse 패키지의 coo_matrix를 이용해 coo 형식으로 최소 행렬 생성\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
      ],
      "metadata": {
        "id": "BamfFc4k_dGp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_coo.toarray()\n",
        "# back to origin array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQBzONXGBVq1",
        "outputId": "96e15aef-9eb3-419f-eb2f-3741a95075eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 희소 행렬 - CSR 형식\n",
        "\n",
        "coo배열에서 행위치 배열에도 순차적인 같은 값이 반복되는 것을 알 수 있는데 행위치 배열이 0부터 순차적으로 증가한느 값으로 이뤄졌다는 특성을 고려하면 행 위치 배열의 고유한 값의 시작위치만 표기하는 방법으로 이러한 반복을 제거할 수 있다. \n",
        "\n",
        "행 위치 배열 [0. 0, 1, 1. 1, 1, 1.2. 2. 3. 4. 4. 5] 를 csr로 변환하면[0, 2. 7. 9. 10. 12]가 된다.\n",
        "그리고 맨 마지막에는 데이터의 총 항목 개수를 배열에 추가한다.\n",
        "\n",
        "이는 사이파이의 csr_matrix를 통해 쉽게 할 수 있다."
      ],
      "metadata": {
        "id": "GbWm9IlXBcZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "dense2 = np.array([[0,0, 1, 0,0, 5], [1, 4, 0, 3, 2, 5], [0, 6, 0,3, 0, 0],[2, 0, 0, 0, 0,0],[0, 0, 0, 7, 0, 8], [1, 0, 0, 0, 0, 0]])\n",
        "\n",
        "# 0이 아닌 데이터 추출\n",
        "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
        "\n",
        "# 행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
        "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
        "\n",
        "# coo 형식 변환\n",
        "sparse_coo =sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
        "\n",
        "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
        "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
        "\n",
        "# CSR 형식으로 변환\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print('coo 변환 데이터가 제대로 되었는지 다시 dense로 출력확인')\n",
        "print(sparse_coo.toarray())\n",
        "print('csr 변환 데이터가 제대로 되었는지 다시 dense로 출력확인')\n",
        "print(sparse_csr.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2sBIl86CAvx",
        "outputId": "cfc234d7-9b46-4811-9c5d-f0f8d651ca69"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coo 변환 데이터가 제대로 되었는지 다시 dense로 출력확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "csr 변환 데이터가 제대로 되었는지 다시 dense로 출력확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 사용시는 밀집행렬을 생성 파라미터로 입력하면 coo or csr 희소 행렬로 자동 생성\n",
        "dense3 = np.array([[0,0,1,0,0,5],\n",
        "                  [1,4,0,3,2,5],\n",
        "                  [0,6,0,3,0,0],\n",
        "                  [2,0,0,0,0,0],\n",
        "                  [0,0,0,7,0,8],\n",
        "                  [1,0,0,0,0,0]])\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)"
      ],
      "metadata": {
        "id": "psusFsJfDhQZ"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}