{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamburins/ESAA_2023/blob/main/%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3_%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8_%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%89%E1%85%B3%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 텍스트 분석 연습 문제**\n",
        "\n",
        "- 출처 : 캐글"
      ],
      "metadata": {
        "id": "Yw5mfB-1YfRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tokenization**\n",
        "\n",
        "In the field of Natural Language Processing, tokenization basically refers to splitting up a larger body of text into smaller lines or words.\n",
        "\n",
        "There are mainly two types of tokenization :\n",
        "\n",
        "- Sentence Tokenization\n",
        "- Word Tokenization"
      ],
      "metadata": {
        "id": "zZBGXubsY6lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "metadata": {
        "id": "zpux756aZRgB",
        "outputId": "c443e8e2-a43d-415f-dbd8-eb025d0d36fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text to perform our operations\n",
        "text = \"Hi, My name is Amartya Nambiar. I am a Computer Science Engineer. My favourite color is black\""
      ],
      "metadata": {
        "id": "-vKDqW1WZcjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sentences=sent_tokenize(text=text)"
      ],
      "metadata": {
        "id": "GowligokZeEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화, 길이 출력\n",
        "words=word_tokenize(text)\n",
        "print(len(words))"
      ],
      "metadata": {
        "id": "pY1VFCkVaDrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4052d9b-3eb1-4e9c-db83-b7d65add8b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "['Hi', ',', 'My', 'name', 'is', 'Amartya', 'Nambiar', '.', 'I', 'am', 'a', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'is', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Stopwords & Flushing them**\n",
        "\n",
        "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
      ],
      "metadata": {
        "id": "unjlrUQTaiGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords  "
      ],
      "metadata": {
        "id": "Rf5p8-7KazcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6291d2c1-bf3f-4706-8498-ccadeebd3b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english stopword 불러오기, 15개만 확인\n",
        "print(nltk.corpus.stopwords.words('english')[:15])"
      ],
      "metadata": {
        "id": "f8kqXiktbBSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1b1020-a337-43e6-b048-6f1b2cf598db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필터링을 통해 text에서 stopword 제거\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens=[]\n",
        "\n",
        "for word in words:\n",
        "  word=word.lower()\n",
        "  if word not in stopwords:\n",
        "    all_tokens.append(word)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "id": "CTeQujmRbPZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf6b0d8-ede6-4530-c6f6-73d0d1ccec04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', ',', 'My', 'name', 'Amartya', 'Nambiar', '.', 'I', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# punctuation('.', ',') 제거\n",
        "pun = ['.',',']\n",
        "fin_tokens=[]\n",
        "\n",
        "for word in all_tokens:\n",
        "  if word not in pun:\n",
        "    fin_tokens.append(word)\n",
        "\n",
        "print(fin_tokens)"
      ],
      "metadata": {
        "id": "lxAH2ytMb3TY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05e7085-189e-4883-cd69-a41083940e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'My', 'name', 'Amartya', 'Nambiar', 'I', 'Computer', 'Science', 'Engineer', 'My', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Stemming**\n",
        "\n",
        "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
        "\n",
        "There are mainly two widely used Stemmer Algorithms:\n",
        "\n",
        "- Porter Stemmer (we'll work on this)\n",
        "- Lancaster Stem"
      ],
      "metadata": {
        "id": "yEAlzfMhcBcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "EQQuNe0McNBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ps 객체 생성 후 stemming , example 최소 3개 임의 생성 후 시도해보기\n",
        "ps=PorterStemmer()\n",
        "# example1= ['helps', 'helping', 'helped']\n",
        "print(ps.stem('helps'), ps.stem('helping'), ps.stem('helped'))\n",
        "print(ps.stem('do'), ps.stem('did'), ps.stem('done'))\n",
        "print(ps.stem('break'), ps.stem('broke'), ps.stem('broken'))\n"
      ],
      "metadata": {
        "id": "hp_atqYwdkR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5799587a-b77b-4749-da2d-d7e6f28c731d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "help help help\n",
            "do did done\n",
            "break broke broken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('happiness') # but it isn't always the best choice"
      ],
      "metadata": {
        "id": "mwMDJ3ZaduyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5b66b68a-bf95-4179-9364-105ac08b06ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Parts of Speech**\n",
        "\n",
        "To know what is the context of a particular word\n",
        "\n",
        "For example : Shyam is a Proper Noun, Desk is a Noun and Happy is an adjective."
      ],
      "metadata": {
        "id": "71eO1YE1dwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#오류\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import movie_reviews\n",
        "#text = movie_reviews.raw(\"neg/cv954_19932.txt\")"
      ],
      "metadata": {
        "id": "S2iVOPi4eEKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "nltk.download('movie_reviews')\n",
        "text = movie_reviews.raw(\"neg/cv954_19932.txt\")\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xzo9JswKvNF",
        "outputId": "8e92d57d-ea62-4b52-bc5e-08a301750c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply pos_tag(), print result\n",
        "pos = pos_tag(word_tokenize(text))\n",
        "pos[1:5]   "
      ],
      "metadata": {
        "id": "vcWgmScAedLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b60e1e-1d1b-4824-a11c-5c9c4240b369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('new', 'JJ'), ('entry', 'NN'), ('in', 'IN'), ('the', 'DT')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Lemmatization**\n",
        "\n",
        "PorterStemmer class chops off the suffixes from the word but this isn't the best thing to apply to clean our data.\n",
        "\n",
        "Stemming technique only looks at the form of the word whereas Lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
      ],
      "metadata": {
        "id": "NrFml-IJepQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "xWF0Ibznetk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed9aff7-1df0-4e76-c852-b72e76dfbfeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize 'believes', 'happiness'\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('believes', 'v'), lemma.lemmatize('happiness','a'))\n"
      ],
      "metadata": {
        "id": "C-abWCqffiwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b938a102-2420-4aea-8dc7-522a0bd1c768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "believe happiness\n"
          ]
        }
      ]
    }
  ]
}