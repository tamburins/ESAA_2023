{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hWR40571ioYcmyqmsoSMp7XdutIPKx14",
      "authorship_tag": "ABX9TyNWf+gijZUR7iIawFRp+yus",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamburins/ESAA_2023/blob/main/ESAA_6_19(201_228).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05 개/고양이 분류\n",
        "파일형태의 데이터셋을 분석에 활용하기 위해서는 고성능 gpu가 탑재된 워크스테이션이나 피씨에 저장된 데이터셋을 텐서플로 분석환경으로 읽어오는 과정이 필요하다. 하지만 실무에서는 컴퓨터 메모리 또는 그래픽 카드 메모리가 부족한 상황이 발생하기 쉽상이다.\n",
        "\n",
        "따라서 데이터셋을 배치단위로 나눈 다음 배치 한개를 읽어와 딥러닝 모델에 주입하면 메모리부담없이 학습할 수 있다. 전체 데이터셋을 전부 모델에 입력할 때까지 배치 단위로 읽어오고 주입한느 과정을 반복한다.\n",
        "\n",
        "이번에는 데이터셋을 배치단위의 여러 부분으로 나누고 반복 객체를 통해 각 배치를 한개씩 모델에 입력하여 훈련하는 방법을 소개한다. 이 과정을 손쉽게 처리하기 위해 텐서플로 케라스는 이미지 데이터 제너레이터 함수를 지원한다.\n",
        "\n",
        "## 5-1 분석준비\n",
        "### 5-1-1 데이터셋 다운로드\n",
        "### 5-1-2 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "dZv5XaVPPXFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "NJ4Oa1vQMi6D",
        "outputId": "a98a5c81-e8b1-4374-b393-303c981dc1ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a72105b2a594>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflowkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflowkeras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflowkeras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 훈련 및 50에폭 훈련\n",
        "tc_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train model\n",
        "tc_history=tc_model.fit(train_aug, validation_data=valid_aug, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "C_SkE6THRue5",
        "outputId": "d2ba28f0-0892-43ec-b280-04ae814a65d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-16741405e4e6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#모델 훈련 및 50에폭 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 하다가 컴퓨터가 먹통이 되어 에폭을 줄였읍니다..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tc_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss acc\n",
        "def plot_loss_acc(history, epoch):\n",
        "\n",
        "    loss, val_loss = history.history['loss'], history.history['val_loss']\n",
        "    acc, val_acc = history.history['accuracy'], history.history['val_accuracy']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    axes[0].plot(range(1, epoch + 1), loss, label='Training')\n",
        "    axes[0].plot(range(1, epoch + 1), val_loss, label='Validation')\n",
        "    axes[0].legend(loc='best')\n",
        "    axes[0].set_title('Loss')\n",
        "\n",
        "    axes[1].plot(range(1, epoch + 1), acc, label='Training')\n",
        "    axes[1].plot(range(1, epoch + 1), val_acc, label='Validation')\n",
        "    axes[1].legend(loc='best')\n",
        "    axes[1].set_title('Accuracy')\n",
        "\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "ULva-qI4ciX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_acc(tc_history, 50)"
      ],
      "metadata": {
        "id": "LfQOoZU6SYat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 초기부터 검증셋에 대한 정확도가 80을 넘어간다. 즉 사전학습 모델인 resnet을 사용하니 이미지로부터 피처를 빠르게 ㄹ추출하는 것을 확인된다.\n",
        "\n",
        "### 5-1-3 구글 드라이브 마운트\n",
        "구글 드라이브 마운트하기\n",
        "\n",
        "### 5-1-4 압축 파일 해제\n",
        "파일경로를 source filename에 지정 후 압축 해제한 파일을 저장할 경로를 저장한다."
      ],
      "metadata": {
        "id": "NSpaNyHRShIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = ''\n",
        "source_filename=drive_path+''\n",
        "extract_folder = 'dataset/'\n",
        "import shutil\n",
        "shutil.unpack_archive(source_filename, extract_folder)"
      ],
      "metadata": {
        "id": "rfssQz8FUY4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 셋 검증 셋 저장 위치 지정\n",
        "train_dir = extract_folder+'archive/training_set/training_set'\n",
        "valid_dir = extract_folder+'archive/test_set/test_set'\n",
        "print(train_dir)\n",
        "print(valid_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "r5KP59ZkUtep",
        "outputId": "18156cea-f7e7-4fb0-fb99-f0b2c41f2239"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-70a325a7b342>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 훈련 셋 검증 셋 저장 위치 지정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'archive/training_set/training_set'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'archive/test_set/test_set'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_folder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5-2 모델 학습\n",
        "### 5-2-1 imagedatagenerator\n",
        "imagedatagenerator 함수를 실행하고 rescale 옵션을 지정해 이미지 각 픽셀의 값을 0-1로 정규화하고 객체를 image gen 변수에 할당한다"
      ],
      "metadata": {
        "id": "yWChM4IjU-tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_gen=ImageGenerator(rescale=(1/255.))\n",
        "image_gen"
      ],
      "metadata": {
        "id": "2-bS1pPlU7sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-2-2 flow from directory 함수\n",
        "지정 폴더에서 이미지를 가져와 반복 이터레이션이 가능하도록 데이터셋을 처리한다.\n",
        "\n",
        "훈련셋이 저장되어 잇는 Train dir, batchsize 속성에는 배치를 구성하는 이미지개수 32, 타겟사이즈 속성에는 저장될 이미지의 픽셀사이즈, 클래스 속성에는 클레스 레이블, 클래스 모드에는 이진분류 문제를 나타내는 binary 모드 ,랜덤시드값을 지정한다.\n",
        "\n",
        "예를 들어 cats 폴더에 들어있는 고양이 이미지를 224, 224, 크기로 리사이징 하고 클래스 레이블은 cats에 해당하는 정수로 레이블 인코딩한다. 이밎를 32장씩 묶어 하나으 ㅣ배치를 구성한다. 검증셋에 대해서도 동일 방식으로 제너레이터 객체를 만든다."
      ],
      "metadata": {
        "id": "d85QaoIwWrTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = image_gen.flow_from_directory(train_dir, batch_size=32, target_size=(224,224), classes=['cats','dogs'], class_mode='binary', seed=2020)\n",
        "valid_gen = image_gen.flow_from_directory(valid_dir, batch_size=32, target_size=(224,224), classes=['cats','dogs'], class_mode='binary', seed=2020)\n"
      ],
      "metadata": {
        "id": "LNIGJQYBYAs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하나으 ㅣ배치를 선택해 배치안에 들어있는 32개의 이미지를 정답클래스 레이블과 함께 출력하면 다음과 같다\n",
        "class_labels=['cats','dogs']\n",
        "batch=next(train_gen)\n",
        "images, labels=batch[0], batch[1]\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "for i in range(32):\n",
        "  ax=plt.subplot(4,8,i+1)\n",
        "     plt.imshow(images[i])\n",
        "     plt.titles(class_labels[labels[i].astype(np.int)])\n",
        "     plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "phgU9na6YV5m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-2-3 모델 훈련\n",
        "이진 분류모델을 정의하고 배치정규화 합성곱 풀링으로 구성된 단위브럭을 3개 반복하여 이미지로부터 다양한 피처를 추출하고 최종 분류기로는 덴스 레이어를 사용한다. 최종 출력 레이어는 노드 1개를 갖고 활성화 함수로는 시그모이드를 적용한다."
      ],
      "metadata": {
        "id": "NXCVcNTSY7lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  model=tf.keras.Sequential([\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Conv2D(32,(3,3), padding='same', activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Conv2D(64,(3,3), padding='same', activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Conv2D(128,(3,3), padding='same', activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(256, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0,5),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "  ])\n",
        "  return model\n",
        "model=build_model()"
      ],
      "metadata": {
        "id": "Xh7IhfxCYZ57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "옵티마이저 손실함수 지정 후 모델을 컴파일하고 20에폭동안 훈련시킨다. 훈련셋에 대한 정확도는 92, 검증 셋에 대한 정확도는 79로 과대적합이 발생하였다."
      ],
      "metadata": {
        "id": "JZ08Nd-8Z-k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(lr=0.001),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history=model.fit(train_gen, validation_dsata=valid_gen, epochs=20)"
      ],
      "metadata": {
        "id": "-o2sAK7VaEm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss acc"
      ],
      "metadata": {
        "id": "z7oqqd1wbdV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실함수 그래프에서도 과대적합을 확인 가능\n",
        "plot_loss_acc(history,20)"
      ],
      "metadata": {
        "id": "8A2qMZ9-bX-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-2-4 데이터 증강\n",
        "image data generator를 사용하면 데티터증강기법을 클래스함수의 매개변수 속성으로 지정할 수 있어 편리하다.\n",
        "\n",
        "좌우반전의 Horizontal flip, 반시계 방향으로 밀리도록 하는 shear range, 줌으로 확대하는 zoom range 속성을 추가한다.\n",
        "\n",
        "저장되어있는 폭더에서 이미지를 배치단위로 불러와 데이터 증강기법을 적용하고 목표 크기에 맞춰 미니배치를 구성한다. 이르 모아 반복 이터레이션 객체를 만든다.\n",
        "\n",
        "모델 인스턴스를 생성하고 훈련한다. 에폭을 40으로 늘리는데 다행히 과대적합이 발생하지 않았다. 에폭을 늘리면 모델 성능ㅇ ㅣ개선될 여지가 남아있다."
      ],
      "metadata": {
        "id": "jda3uuCVbcwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_gen_aug = ImageDataGenerator(rescale=1/255., horizontal_fliop=True, rotation_range=30, shear_range=0.15, zoom_range=0.3)\n",
        "\n",
        "train_gen_aug = image_gen_aug.flow_from_directory(train_dir, batch_size=32, target_size=(224,224), classes=['cats', 'dogs'],class_mode='binary', seed=2020)\n",
        "valid_gen_aug = image_gen_aug.flow_from_directory(valid_dir, batch_size=32, target_size=(224,224), classes=['cats', 'dogs'],class_mode='binary', seed=2020)"
      ],
      "metadata": {
        "id": "9QOFcnLHb8XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_aug=build_model()\n",
        "model_aug.compile(optimizer=tf.optimizers.Adam(lr=0.001),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history_aug=model.fit(train_gen, validation_dsata=valid_gen, epochs=20)"
      ],
      "metadata": {
        "id": "bFZaBAFkcm20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 과대적합 과소적합 거의 없이 학습이 잘 진행된다!\n",
        "plot_loss_auc(history_aug, 20)"
      ],
      "metadata": {
        "id": "5M-WzyPtcwrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06 객체 탐지\n",
        "\n",
        "객체 탐지 혹은 객체 검출은 이미지를 인식하는 컴퓨터 비전 ai 기술이 가장 많이 응용되는 분야로 가장 대표적인 응용 분야로 자율주행차가 대표적인 활용 예시이다. 이는 카메라 또는 센서를 활용하여 도로상황을 파악하여 장애물 사람을 식별하고 다른 자동차의 움직임을 파악한다.\n",
        "\n",
        "객체 탐지는 입력이미지로부터 여러개의 객체를 찾아내고 각 객체가 무엇을 나내는지 분류하는 두가지 작업을 처리한다. 이미지에서 각 개체의 좌표값을 회귀문제로 접근한다. 위치를 찾아낸 각 객체가 어떤 클래스에 속하는지 분류하는 문제를 다음에 처리한다.\n",
        "\n",
        "즉 이미지를 인식해 이미지 안에 들어있는 여러 객체의 위치를 찾는 회귀문제와 찾아낸 객체를 분류하는 문제가 결합되어 있다.\n",
        "\n",
        "## 6-1 텐서플로 허브 활용\n",
        "텐서플로 허브는 검증된 사전학습 모델을 제공하는 저장ㅎ소로 모델을 배포하여 그댈로 서빙하는 것도 가능하며 전이학습을 거쳐 개별 도메인에 맞게 튜닝 후 배포하는 것도 가능하다.\n",
        "\n",
        "이번에는 텐서플로 허브에서 제공하는 객체탐지 모델을 사용하여 샘플 이미지로부터 객체를 추출하는 작업을 한다. 먼저 허브 라이브러리를 직접 불러온다."
      ],
      "metadata": {
        "id": "v_A9FTXIcvsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as tfhub\n"
      ],
      "metadata": {
        "id": "NPx6XNZIgtG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-1-1 샘플이미지 준비\n",
        "이미 학습이 완료된 딥러닝 모델을 사용하므로 별도 모델학습을 진행하지 않는다.\n",
        "샘플 이미지를 준비하여 진행한다."
      ],
      "metadata": {
        "id": "B0S3VdmKgxoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "img_path = 'https://upload,wikimedia.org/wikipedia/commons/thumb/c/c4/Gangnam_Seoul_January_2009.jpg/1280px-Gangnam_Seoul_January_2009.jpg'\n",
        "img = tf.keras.utils.get_file(fname='gangnam', origin=img_path)\n",
        "\n",
        "img=tf.io.read_file(img)\n",
        "img=tf.image.decode_jpeg(img, channels=3)\n",
        "img=tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "SbTOT0ytg8Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4차원 텐서로 입력받는 사전학습모델\n",
        "img_input = tf.expand_dims(img, 0)\n",
        "img_input.shape"
      ],
      "metadata": {
        "id": "I4-iET3shovb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-1-2 사전학습모델\n",
        "\n",
        "inception resnet v2모델을 사용한다\n",
        "\n",
        "inception resnet v2 모델의 링크를 클릭하여 선택하면 다음과 같은 화면이 나타나는데 url을 복사하면 모델을 가져올 링크가 복사된다.\n",
        "\n",
        "앞서 가져온 링크를 Load 함수에 전달해주면 모델을 불러온다. 모델 변수에 저장한다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iphFhIvShxsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #TenSOrP1owHub에서모델가져오기- Past erRCNN+InceptionResNetV2\n",
        "model = tfhub.load('https://tfhub.dev/google/faster_rcnn/openimages_v4/ inception_resnet _v2/1')"
      ],
      "metadata": {
        "id": "lKonEvaWiHg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check signature of model\n",
        "model.signatures.keys()"
      ],
      "metadata": {
        "id": "ff2CClgpiLbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set default, make instance\n",
        "obj_detector = model.signatures['default']\n",
        "obj_detector"
      ],
      "metadata": {
        "id": "DU7enDGniQBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-1-3 추론\n",
        "객체탐지 모델에 앞서 미리 전처리를 통해 준비한 샘플이미지를 입력한다. 모델은 추론을 거쳐 예측값을 반환한다. 딕셔너리 키 배열을 확인하면 다음과 같다"
      ],
      "metadata": {
        "id": "zqkb9kzqiYW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result=obj_detector(img_input)\n",
        "result.keys()"
      ],
      "metadata": {
        "id": "ogFyoPCIiefW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이들 중 경계박스좌표, 검출된 클래스 아이디, 검출스코어를 사용하여 검출스코어 개수를 확인한 결과 100개의 객체를 탐지한 것을 알 수 있다."
      ],
      "metadata": {
        "id": "zPQ6kCsqikuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(result['detection_scores'])"
      ],
      "metadata": {
        "id": "sCLDCGRQiq0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 객체 탐지 결과를 시각화\n",
        "boxes = result[\"detection_boxes\"]    # Bounding Box 좌표 예측값\n",
        "labels = result[\"detection_class_entities\"]   # 클래스 값\n",
        "scores = result[\"detection_scores\"]   # 신뢰도 (confidence)\n",
        "\n",
        "# 샘플 이미지 가로 세로 크기\n",
        "img_height, img_width = img.shape[0], img.shape[1]\n",
        "\n",
        "# 탐지할 최대 객체의 수\n",
        "obj_to_detect = 10\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(min(obj_to_detect, boxes.shape[0])):\n",
        "    if scores[i] >= 0.2:\n",
        "        (ymax, xmin, ymin, xmax) = (boxes[i][0]*img_height, boxes[i][1]*img_width,\n",
        "                                    boxes[i][2]*img_height, boxes[i][3]*img_width)\n",
        "\n",
        "        plt.imshow(img)\n",
        "        plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin],\n",
        "                 color='yellow', linewidth=2)\n",
        "\n",
        "        class_name = labels[i].numpy().decode('utf-8')\n",
        "        infer_score = int(scores[i].numpy()*100)\n",
        "        annotation = \"{}: {}%\".format(class_name, infer_score)\n",
        "        plt.text(xmin+10, ymax+20, annotation,\n",
        "                 color='white', backgroundcolor='blue', fontsize=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ek4fCPjliuHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-2 yolo 객체 탐ㅁ지\n",
        "darknet의 yolo객체 탐지 모델을 사용하여 경계박스오 ㅏ예측 클래스를 서로 다른 문제로 다루지 않고 하나의 회귀문제로 접근하는 개념이다. 즉 하나의 신경망이 한번만 계산해 두가지 잏을 모두 처리한다.\n",
        "\n",
        "### 6-2-1 darknet yolo 모델 추론\n",
        "사전 모델 학습을 활용한다.\n",
        "깃허브 저장소를 코랩 환경으로 다운 후 샘플이미지를 업로드하고 업로드해 파일을 선택하여 업로드한다."
      ],
      "metadata": {
        "id": "2U3gC-oVi7zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AlexeyAB/darknet\n"
      ],
      "metadata": {
        "id": "y6ZHISpYjYFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpu 사용을 위해 darknet의 마크 파일 수정 및 다크넷 생성"
      ],
      "metadata": {
        "id": "Ax9HJmSbje6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# GPU 활성화\n",
        "%cd darknet\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
        "\n",
        "# Darknet 생성\n",
        "!make\n",
        ""
      ],
      "metadata": {
        "id": "vkwd8IN8jfP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 가중치 가져오기\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3\n"
      ],
      "metadata": {
        "id": "Dt-CyXZ1jrY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플이미지 출력 및 확인\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "img = tf.io.read_file('/content/drive/MyDrive/data/gangnam.jpg')\n",
        "img = tf.image.decode_jpeg(img, channels=3)\n",
        "img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "DaEoQvZdjsye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다크넷 실행 및 샘플이미지에 대한 객체탐지 ㅜ론\n",
        "# 객체추출 및 예측확률 계산\n",
        "!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights /content/drive/MyDrive/data/gangnam.jpeg\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "img = tf.io.read_file('/content/darknet/predictions.jpg')\n",
        "img = tf.image.decode_jpeg(img, channels=3)\n",
        "img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "ibvUuy10jxya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-2-2 나만의 yolo 모델 생성\n",
        "간단하게 검정 바탕에 도형 3개만 탐지하는 모델을 만들어 모델을 쉽게 이해해보자!\n",
        "앞의 논문에서는 이미지를 가로 세로 7개의 셀로 나누어 총 49셀을 기본으로 하지만 우리는 가로 세로 3개의 셀로 나누는 방식을 채택한다. 또한 논문에서 한 셀당 2개의 박스를 그리지만 우리는 1개으 ㅣ박스를 그리며 객체종류도3개로 단순화하여 구현한다."
      ],
      "metadata": {
        "id": "gHUNkc7zlFoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "width_size = 256\n",
        "height_size = 256\n",
        "channel_size = 3\n",
        "img_size = (width_size,height_size,channel_size)\n",
        "\n",
        "# 이미지를 나눌 크기\n",
        "cell_num = 3\n",
        "\n",
        "# 찾고자 하는 객체 개수\n",
        "class_num = 3\n",
        "\n",
        "# 한셀에 그릴 박스 수\n",
        "anchor_num = 1\n",
        "label_num = anchor_num * (5 + class_num)\n",
        "\n",
        "# 학습 수\n",
        "epoch_num = 20000\n",
        "\n",
        "# 로스 비중\n",
        "loss_p_rate = 1.0\n",
        "loss_cod_rate = 5.0\n",
        "loss_c_rate = 1.0\n",
        "loss_p_no_rate = 0.5"
      ],
      "metadata": {
        "id": "uMbofYSwlXbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습을 위해 자료실에서 제공하는 3개의 이미지파일을 사용한다. 업로드 버튼을 이용해 코랩 폴더에 도형파일을 업로드한다.\n",
        "\n",
        "CV2를 이용하여 랜덤위치에 도형 3개를 그리고 위치를 찾아 경계박스로 나타내며 정답 클래스 레이블까지 반환하는 함수를 정의한다."
      ],
      "metadata": {
        "id": "L9RzYWYSljYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_img_label():\n",
        "    img = np.zeros((height_size + 400, width_size + 400, channel_size))\n",
        "    label = np.zeros((cell_num, cell_num, label_num))\n",
        "    num_shape = np.random.randint(1,4)\n",
        "    i = np.random.choice(range(cell_num), num_shape, replace=False)\n",
        "    j = np.random.choice(range(cell_num), num_shape, replace=False)\n",
        "\n",
        "    img_0 = cv2.imread('/content/0.png')\n",
        "    img_1 = cv2.imread('/content/1.png')\n",
        "    img_2 = cv2.imread('/content/2.png')\n",
        "\n",
        "    for n_h in range(num_shape):\n",
        "        row = i[n_h]\n",
        "        col = j[n_h]\n",
        "        shape_type = np.random.randint(0, class_num)\n",
        "        x_rate = np.random.rand()\n",
        "        y_rate = np.random.rand()\n",
        "        w_rate = np.random.rand() * 0.3 + 0.1\n",
        "        h_rate = np.random.rand() * 0.3 + 0.1\n",
        "\n",
        "        label[row, col] = [1, x_rate, y_rate, w_rate, h_rate, 0, 0, 0]\n",
        "        label[row, col, 5 + shape_type] = 1\n",
        "        x = int(x_rate * width_size / cell_num + col * width_size / cell_num)\n",
        "        y = int(y_rate * height_size / cell_num + row * height_size / cell_num)\n",
        "        w = int(w_rate * width_size / 2) * 2\n",
        "        h = int(h_rate * height_size / 2) * 2\n",
        "        if(shape_type == 0):\n",
        "            input_img = cv2.resize(img_0, (w,h))\n",
        "        if(shape_type == 1):\n",
        "            input_img = cv2.resize(img_1, (w,h))\n",
        "        if(shape_type == 2):\n",
        "            input_img = cv2.resize(img_2, (w,h))\n",
        "        img[y-int(h/2)+200 : y+int(h/2)+200, x-int(w/2)+200 : x+int(w/2)+200] = input_img\n",
        "    img = img[200 : 200+height_size, 200 : 200+width_size]\n",
        "\n",
        "    return img, label"
      ],
      "metadata": {
        "id": "agtr98rRltCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "객체 탐지 모델의 성능의 일정수준에 도달하기위해서는 복잡한 구조로 구현되어야 한다. 전이학습 방법을 적용하여 이미지 특징을 추춣하는데 좋은 성능을 갖는 모델을 기본으로 활용하는 것이 좋다.\n",
        "\n",
        "다음 코드에서는 vgg16 모델을 베이스로 사용하고 conv2d 층과 dense레이어를 마지막 객체 탐지 분류기로 설정해준다. 모델 구조를 요약하여 확인한다."
      ],
      "metadata": {
        "id": "nRm7wmZGsHaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = tf.keras.applications.VGG16(include_top=False, input_shape=img_size)\n",
        "vgg_model.trainable=False\n",
        "i=tf.keras.Input(shape=img_size)\n",
        "out=tf.keras.layers.Conv2D(256, 3, padding='same')(out)\n",
        "out=tf.keras.layers.Conv2D(128, 3, padding='same')(out)\n",
        "out=tf.keras.layers.Conv2D(64, 3, padding='same')(out)\n",
        "out-tf.keras.layers.Flatten()(out)\n",
        "out=tf.keras.layers.Dense(1024, activation='relu')(out)\n",
        "out=tf.keras.layers.Dense(3*3*8, activation='sigmoid')(out)\n",
        "out=tf.keras.layers.Reshape((3,3,8))(out)\n",
        "yolo_model = tf.kears.Model(inputs=[i], outputs=[out])\n",
        "opt=tf.keras.optimizers.Adam(0.00001)\n",
        "\n",
        "yolo_model.summary()"
      ],
      "metadata": {
        "id": "SCphNneBsUE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지를 총 9개의 셀로 나누고 셀마다 학습을 진행한다. 객체가 있는 셀의 경우 확률박스위치및크기 클래스 종류 ㅁ모두 학습을 진행하고 객채게 앖는 셀은 객체없는 확률만 학습한다. 각 로스는 미리 정한 비중을 곱하고 전체를 더해 최종 로스를 만들어 모델을 학습한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "LTW62fjXw8ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "foc = cv2.VideoWriter_focrcc(*'DIVX')\n",
        "out = cv2.VideoWriter('hjk_yolo.avi', fcc,1.0(width_size, hight_size))\n",
        "for e in range(epoch_num):\n",
        "  img, label=make_img_label()\n",
        "  img = np.reshape(img.(i, hight_size,width_size, 3))\n",
        "  label = np.reshape(label, (1,3,3,8))\n",
        "  lost_p_list=[]\n",
        "  lost_cod_list=[]\n",
        "  loss_c_list=[]\n",
        "  loss_p_no_list=[]\n",
        "  with tf.GradientTape() as tape:\n",
        "    pred = yolo_model(img)\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "        if(label[0.i,j,0]==1):\n",
        "          loss_p_list.append(tf.square(label[0,i,j,0]- pred[0,i,j,0]))\n",
        "          loss_cod_list.append(tf.square(label[0,i,j,1]- pred[0,i,j,1]))\n",
        "          loss_cod_list.append(tf.square(label[0,i,j,2]- pred[0,i,j,2]))\n",
        "          loss_cod_list.append(tf.square(label[0,i,j,3]- pred[0,i,j,3]))\n",
        "          loss_cod_list.append(tf.square(label[0,i,j,4]- pred[0,i,j,4]))\n",
        "          loss_c_list.append(tf.square(label[0,i,j,5]- pred[0,i,j,5]))\n",
        "          loss_c_list.append(tf.square(label[0,i,j,6]- pred[0,i,j,6]))\n",
        "          loss_c_list.append(tf.square(label[0,i,j,7]- pred[0,i,j,7]))\n",
        "        else:\n",
        "          loss_p_no_list.append(tf.square(label[0,i,j,0]- pred[0,i,j,0]))\n",
        "    loss_p=tf.keras.reduce_mean(loss_p_list)\n",
        "    loss_cod = tf.reduce_mean(loss_cod_list)\n",
        "    loss_c=tf.reduce_mean(loss_c_list)\n",
        "    loss_p_no = tf.reduce_mean(loss_p_no_list)\n",
        "\n",
        "    loss=loss_p_rate * loss_p + loss_cod_rate*loss_cod + loss_c_rate*loss_c + loss_p_no_rate*loss_p_no\n",
        "  vars = yolo_model.trainable_variables\n",
        "  grad = tape.gradient(loss, vars)\n",
        "  opt.apply_gradients(zip(grad, vars))\n",
        "\n",
        "  if(e%100==0):\n",
        "    img=np.reshape(img, (256, 256, 3))\n",
        "    label=pred.numpy()\n",
        "    label=np.reshape(label, (3,3,8))\n",
        "    sample_img = np.uint8(show_box(img, label))\n",
        "    out.write(sample_img)\n",
        "  print(e, '완료',loss.numpy())\n",
        "out.release()"
      ],
      "metadata": {
        "id": "K3FwIJ1jxIHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 과정은 동영상 파일로 저장된다. 코랩폴더에서 동영상 파일을 로컬피시로 다운한다. 동영상을 실행해보면 약 만번 학습이 이뤄진 후 객체탐지를 어느정도 수준에서 잘 해는걸 볼 수 있다."
      ],
      "metadata": {
        "id": "2d81rXBez6UG"
      }
    }
  ]
}